# -*- coding: utf-8 -*-
"""Copy of lab2python.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-6UFjv8-AFRrZEk_2LVfVGPoBEmmaCW3
"""

!pip install python-terrier

import pyterrier as pt
if not pt.started():
   pt.init()

!pip install Arabic-stopwords

import pandas as pd
pd.set_option('display.max_colwidth', 150)
import re
 
from snowballstemmer import stemmer

import arabicstopwords.arabicstopwords as stp

    #make your loops show a smart progress meter 

from tqdm import tqdm

docs_df = pd.DataFrame([ 

                        ["d0", "هذا هو المعمل الثاني من دورة استرجاع المعلومات"],

                        ["d1", " سنتعلم في هذا اللاب كيفية التعامل مع النصوص العربية "], 

                        ["d2", "اليوم هو 29 سبتمبر 2022 المرافق يوم الخميس 23 من ربيع الاول 1444 "], 

                        ["d3", "في هذا المعمل ستتعرف على طرق تحضير البيانات و تشذيب النصوص العربية"]

                      ],

                        columns=["docID", "raw_text"])


docs_df

stp.stopwords_list()

len(stp.stopwords_list())

#removing Stop Words function

def remove_stopWords(sentence):

    terms=[]

    stopWords= set(stp.stopwords_list())

    for term in sentence.split() : 

        if term not in stopWords :

           terms.append(term)

    return " ".join(terms)


docs_df["Removed_Stop_word_text"]=docs_df["raw_text"].apply(remove_stopWords)

print("***************************************************************************documents after removing stopwords*********************************************************************")

docs_df

#a function to normalize the tweets

def normalize(text):

    text = re.sub("[إأٱآا]", "ا", text)

    text = re.sub("ى", "ي", text)

    text = re.sub("ؤ", "ء", text)

    text = re.sub("ئ", "ء", text)

    text = re.sub("ة", "ه", text)

    return(text)


docs_df["normalized_text"]=docs_df["Removed_Stop_word_text"].apply(normalize)

print("***************************************************************************documents after normalizing*********************************************************************")

docs_df

#specify that we want to stem arabic text

ar_stemmer = stemmer("arabic")

#define the stemming function

def stem(sentence):

    return " ".join([ar_stemmer.stemWord(i) for i in sentence.split()])


docs_df['Stemtext']=docs_df['normalized_text'].apply(stem)

print("***************************************************************************documents after stemming*********************************************************************")

docs_df

"""Excersice"""

from google.colab import files
uploaded=files.upload()

filee=open("lama.txt")
my_file=filee.read()
my_file

import nltk
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

stopwords.words('english')

#remove stop words
filtered_list = []
stop_words = nltk.corpus.stopwords.words('english')
# Tokenize the sentence
words = word_tokenize(my_file)
for w in words:
    if w not in stop_words:
        filtered_list.append(w)
filtered_list

my_clean_txt = " ".join(filtered_list)
my_clean_txt

#normalize
my_clean_txt=my_clean_txt.lower()
my_clean_txt=re.sub("[.,'-_?;:!()=]", "",my_clean_txt )
print(my_clean_txt)

sentences = nltk.sent_tokenize(my_clean_txt)
print(sentences)

#stemming
from nltk.stem import PorterStemmer
ps = PorterStemmer()

for i in range(len(sentences)):
    words = nltk.word_tokenize(sentences[i])
    words = [ps.stem(word) for word in words]
    sentences[i] = ' '.join(words)
print(sentences)